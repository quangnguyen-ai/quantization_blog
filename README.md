# Quantization for Real-Time AI Inference

A technical exploration of neural network quantization techniques, from mathematical foundations to production deployment challenges on edge devices.

## ðŸ“š Content

**[Part 1: Quantization - A Solution for Real-Time Inference](part1/quantization.MD)**

An in-depth article covering quantization fundamentals, mathematical formulation, quantization strategies (weight-only vs. full INT8), and real-world deployment considerations. Includes concrete examples from deploying models on Qualcomm (SNPE/QNN), NVIDIA Jetson (TensorRT), and Renesas platforms, with benchmarks showing 40-70% latency reduction and 4Ã— memory optimization.

---

**Author:** Nguyen Gia Quang | AI Research Engineer
**Focus:** On-device AI optimization, quantization, efficient inference
